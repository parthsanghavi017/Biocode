{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Step 1: Read the CSV file into a DataFrame in chunks\n",
    "data_chunks = pd.read_csv('moses.csv', chunksize=5000)\n",
    "\n",
    "# Step 2: Process each chunk and save to separate CSV files\n",
    "processed_files = []\n",
    "\n",
    "for i, chunk in enumerate(data_chunks):\n",
    "    # Drop the second column ('SPLIT')\n",
    "    chunk.drop(columns=['SPLIT'], inplace=True)\n",
    "    \n",
    "    # Remove duplicates and null values\n",
    "    chunk.drop_duplicates(inplace=True)\n",
    "    chunk.dropna(inplace=True)\n",
    "    \n",
    "    # Save the processed chunk to a separate CSV file\n",
    "    processed_file = f'processed_chunk_{i}.csv'\n",
    "    chunk.to_csv(processed_file, index=False)\n",
    "    \n",
    "    processed_files.append(processed_file)\n",
    "\n",
    "# Step 3: Concatenate the processed chunks into a single DataFrame\n",
    "final_result = pd.concat([pd.read_csv(file) for file in processed_files], ignore_index=True)\n",
    "\n",
    "# Step 4: Drop duplicates from the final result\n",
    "final_result.drop_duplicates(inplace=True)\n",
    "\n",
    "# Step 5: Save the final result to a CSV file\n",
    "final_result.to_csv('moses_processed.csv', index=False)\n",
    "\n",
    "# Step 6: Delete the individual processed chunk files\n",
    "for file in processed_files:\n",
    "    os.remove(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import os\n",
    "\n",
    "# Step 1: Read the processed CSV file into a DataFrame in chunks\n",
    "data_chunks = pd.read_csv('moses_processed.csv', chunksize=5000)\n",
    "\n",
    "# Step 2: Initialize a counter for tracking the processed chunks\n",
    "processed_chunks_count = 0\n",
    "\n",
    "# Step 3: Function to calculate Morgan fingerprints for a chunk and save to CSV\n",
    "def calculate_fingerprints(chunk, chunk_index):\n",
    "    # Convert SMILES to RDKit Mol objects\n",
    "    mols = [Chem.MolFromSmiles(smiles) for smiles in chunk['SMILES']]\n",
    "    \n",
    "    # Calculate Morgan fingerprints\n",
    "    fps = [AllChem.GetMorganFingerprintAsBitVect(mol, 3, 2048) for mol in mols]\n",
    "    \n",
    "    # Initialize an empty list to store fingerprint data\n",
    "    fps_data = []\n",
    "    \n",
    "    # Iterate over each fingerprint\n",
    "    for fp in fps:\n",
    "        # Convert fingerprint object to binary string and split it into individual bits\n",
    "        bits = [int(bit) for bit in fp.ToBitString()]\n",
    "        # Append the bits to the fps_data list\n",
    "        fps_data.append(bits)\n",
    "    \n",
    "    # Create a DataFrame with the fingerprint data\n",
    "    fp_df = pd.DataFrame(fps_data, columns=[f'Bit{i}' for i in range(2048)])\n",
    "    \n",
    "    # Concatenate the original data with the fingerprint DataFrame\n",
    "    result = pd.concat([chunk.reset_index(drop=True), fp_df], axis=1)\n",
    "    \n",
    "    # Save the processed chunk with fingerprints to a separate CSV file\n",
    "    processed_file = f'processed_chunk_with_fingerprints_{chunk_index}.csv'\n",
    "    result.to_csv(processed_file, index=False)\n",
    "    \n",
    "    return processed_file\n",
    "\n",
    "# Step 4: Process chunks and calculate fingerprints\n",
    "processed_files = []\n",
    "\n",
    "for i, chunk in enumerate(data_chunks):\n",
    "    processed_file = calculate_fingerprints(chunk, i)\n",
    "    processed_files.append(processed_file)\n",
    "    processed_chunks_count += 1\n",
    "    print(f'Processed chunk {processed_chunks_count} saved to {processed_file}')\n",
    "\n",
    "# Step 5: Concatenate all processed files into one final CSV file\n",
    "final_result_with_fingerprints = pd.concat([pd.read_csv(file) for file in processed_files], ignore_index=True)\n",
    "final_csv_file = 'moses_with_fingerprints.csv'\n",
    "final_result_with_fingerprints.to_csv(final_csv_file, index=False)\n",
    "print(f'Concatenated all processed files into {final_csv_file}')\n",
    "\n",
    "# Step 6: Delete the individual processed chunk files\n",
    "for file in processed_files:\n",
    "    os.remove(file)\n",
    "    print(f'Deleted {file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# List all CSV files in the directory\n",
    "csv_files = [file for file in os.listdir() if file.startswith('processed_chunk_with_fingerprint')]\n",
    "\n",
    "# Name for the final merged CSV file\n",
    "final_csv_file = 'merged_moses_with_fingerprints.csv'\n",
    "\n",
    "# Check if the final CSV file already exists\n",
    "if os.path.exists(final_csv_file):\n",
    "    os.remove(final_csv_file)\n",
    "\n",
    "# Open the final CSV file in append mode\n",
    "with open(final_csv_file, 'a') as final_csv:\n",
    "    # Iterate over each CSV file\n",
    "    for file in csv_files:\n",
    "        # Open and read each CSV file chunk by chunk\n",
    "        for chunk in pd.read_csv(file, chunksize=10000):  # Adjust chunksize as needed\n",
    "            # Write the chunk to the final CSV file without header after the first chunk\n",
    "            chunk.to_csv(final_csv, index=False, header=not os.path.exists(final_csv_file))\n",
    "\n",
    "        # Optionally, delete the individual file to free up memory\n",
    "        os.remove(file)\n",
    "        print(f\"Processed and removed: {file}\")\n",
    "\n",
    "print(\"All files merged and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Step 1: Read the merged CSV file into a DataFrame in chunks\n",
    "chunk_size = 5000\n",
    "temp_files = []\n",
    "\n",
    "# Step 2: Apply PCA for dimensionality reduction\n",
    "n_components = 100\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "\n",
    "# Step 3: Apply KMeans clustering\n",
    "n_clusters = 100\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Step 4: Process data in chunks\n",
    "for chunk in pd.read_csv('merged_moses_with_fingerprints.csv', header=None, chunksize=chunk_size):\n",
    "    # Extract SMILES as features for PCA\n",
    "    smiles_column = chunk.iloc[:, 0].values\n",
    "    \n",
    "    # Convert SMILES to Morgan fingerprints\n",
    "    mols = [Chem.MolFromSmiles(smi) for smi in smiles_column]\n",
    "    features = [Chem.GetMorganFingerprintAsBitVect(m, 2) for m in mols]\n",
    "    features = np.array([list(feat) for feat in features])\n",
    "    \n",
    "    # Apply PCA\n",
    "    features_reduced = pca.fit_transform(features)\n",
    "    \n",
    "    # Apply KMeans clustering\n",
    "    cluster_labels = kmeans.fit_predict(features_reduced)\n",
    "    \n",
    "    # Write intermediate results to disk\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    np.savez(temp_file, features_reduced=features_reduced, cluster_labels=cluster_labels)\n",
    "    temp_files.append(temp_file.name)\n",
    "\n",
    "# Step 5: Extract 10 molecules from each cluster\n",
    "cluster_molecules = {}\n",
    "\n",
    "for temp_file_name in temp_files:\n",
    "    with np.load(temp_file_name) as data:\n",
    "        features_reduced = data['features_reduced']\n",
    "        cluster_labels = data['cluster_labels']\n",
    "        \n",
    "        for i in range(n_clusters):\n",
    "            cluster_indices = np.where(cluster_labels == i)[0]\n",
    "            \n",
    "            if len(cluster_indices) > 10:\n",
    "                # Ensure random indices are within bounds\n",
    "                valid_indices = [idx for idx in cluster_indices if idx < len(smiles_column)]\n",
    "                random_indices = np.random.choice(valid_indices, 10, replace=False)\n",
    "                \n",
    "                smiles = smiles_column[random_indices]\n",
    "                cluster_molecules[i] = smiles.tolist()\n",
    "            elif len(cluster_indices) > 0:\n",
    "                smiles = smiles_column[cluster_indices]\n",
    "                cluster_molecules[i] = smiles.tolist()\n",
    "\n",
    "# Step 6: Create a DataFrame to store the results\n",
    "results = []\n",
    "\n",
    "for cluster, smiles_list in cluster_molecules.items():\n",
    "    for smiles in smiles_list:\n",
    "        results.append({'SMILES': smiles, 'Cluster': cluster})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Step 7: Save the results to a CSV file\n",
    "results_df.to_csv('clustered_molecules.csv', index=False)\n",
    "\n",
    "# Step 8: Cleanup temporary files\n",
    "for temp_file_name in temp_files:\n",
    "    os.remove(temp_file_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_QSAR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
